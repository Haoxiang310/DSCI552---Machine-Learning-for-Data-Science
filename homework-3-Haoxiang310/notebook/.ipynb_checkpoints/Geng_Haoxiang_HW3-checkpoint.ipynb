{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Geng_Haoxiang_HW3</h1></center>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Haoxiang Geng\n",
    "<br>\n",
    "Github Username: Haoxiang310\n",
    "<br>\n",
    "USC ID: 8045015278"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Time Series Classification Part 1: Feature Creation/Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the AReM Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_paths = {\n",
    "    'bending1': '../data/AReM/bending1',\n",
    "    'bending2': '../data/AReM/bending2',\n",
    "    'cycling': '../data/AReM/cycling',\n",
    "    'lying': '../data/AReM/lying',\n",
    "    'sitting': '../data/AReM/sitting',\n",
    "    'standing': '../data/AReM/standing',\n",
    "    'walking': '../data/AReM/walking'\n",
    "}\n",
    "\n",
    "sniffer = csv.Sniffer()\n",
    "\n",
    "def get_datasets_in_folder(folder_path):\n",
    "    datasets = os.listdir(folder_path)\n",
    "    datasets.sort(key=lambda x: int(x.split('dataset')[1].split('.')[0]))\n",
    "    return datasets\n",
    "        \n",
    "columns = [\"time\", \"avg_rss12\", \"var_rss12\", \"avg_rss13\", \"var_rss13\", \"avg_rss23\", \"var_rss23\"]\n",
    "\n",
    "#fix the error dataset(cycling/dataset9.csv and cycling/dataset14.csv)\n",
    "def fix_error_dataset(dataset_path):\n",
    "    if \"cycling/dataset9.csv\" in dataset_path or \"cycling/dataset14.csv\" in dataset_path:\n",
    "        with open(dataset_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            lines[-1] = lines[-1][:-1]\n",
    "        \n",
    "        with open(dataset_path, 'w') as f:\n",
    "            f.writelines(lines)\n",
    "\n",
    "def read_dataset_file(dataset_path):\n",
    "    fix_error_dataset(dataset_path)\n",
    "    if 'bending2/dataset4' in dataset_path:\n",
    "        delimiter = '\\s+'\n",
    "    else:\n",
    "        delimiter = ','\n",
    "    data = pd.read_csv(dataset_path, sep=delimiter, skiprows=5, header=None, names=columns)\n",
    "    data['label'] = os.path.basename(os.path.dirname(dataset_path))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Test and Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets in train set: ['bending1/dataset3.csv', 'bending1/dataset4.csv', 'bending1/dataset5.csv', 'bending1/dataset6.csv', 'bending1/dataset7.csv', 'bending2/dataset3.csv', 'bending2/dataset4.csv', 'bending2/dataset5.csv', 'bending2/dataset6.csv', 'cycling/dataset4.csv', 'cycling/dataset5.csv', 'cycling/dataset6.csv', 'cycling/dataset7.csv', 'cycling/dataset8.csv', 'cycling/dataset9.csv', 'cycling/dataset10.csv', 'cycling/dataset11.csv', 'cycling/dataset12.csv', 'cycling/dataset13.csv', 'cycling/dataset14.csv', 'cycling/dataset15.csv', 'lying/dataset4.csv', 'lying/dataset5.csv', 'lying/dataset6.csv', 'lying/dataset7.csv', 'lying/dataset8.csv', 'lying/dataset9.csv', 'lying/dataset10.csv', 'lying/dataset11.csv', 'lying/dataset12.csv', 'lying/dataset13.csv', 'lying/dataset14.csv', 'lying/dataset15.csv', 'sitting/dataset4.csv', 'sitting/dataset5.csv', 'sitting/dataset6.csv', 'sitting/dataset7.csv', 'sitting/dataset8.csv', 'sitting/dataset9.csv', 'sitting/dataset10.csv', 'sitting/dataset11.csv', 'sitting/dataset12.csv', 'sitting/dataset13.csv', 'sitting/dataset14.csv', 'sitting/dataset15.csv', 'standing/dataset4.csv', 'standing/dataset5.csv', 'standing/dataset6.csv', 'standing/dataset7.csv', 'standing/dataset8.csv', 'standing/dataset9.csv', 'standing/dataset10.csv', 'standing/dataset11.csv', 'standing/dataset12.csv', 'standing/dataset13.csv', 'standing/dataset14.csv', 'standing/dataset15.csv', 'walking/dataset4.csv', 'walking/dataset5.csv', 'walking/dataset6.csv', 'walking/dataset7.csv', 'walking/dataset8.csv', 'walking/dataset9.csv', 'walking/dataset10.csv', 'walking/dataset11.csv', 'walking/dataset12.csv', 'walking/dataset13.csv', 'walking/dataset14.csv', 'walking/dataset15.csv'] \n",
      "\n",
      "Datasets in test set: ['bending1/dataset1.csv', 'bending1/dataset2.csv', 'bending2/dataset1.csv', 'bending2/dataset2.csv', 'cycling/dataset1.csv', 'cycling/dataset2.csv', 'cycling/dataset3.csv', 'lying/dataset1.csv', 'lying/dataset2.csv', 'lying/dataset3.csv', 'sitting/dataset1.csv', 'sitting/dataset2.csv', 'sitting/dataset3.csv', 'standing/dataset1.csv', 'standing/dataset2.csv', 'standing/dataset3.csv', 'walking/dataset1.csv', 'walking/dataset2.csv', 'walking/dataset3.csv']\n"
     ]
    }
   ],
   "source": [
    "test_data = []\n",
    "train_data = []\n",
    "test_data_paths = []\n",
    "train_data_paths = []\n",
    "\n",
    "for folder, path in folder_paths.items():\n",
    "    datasets = get_datasets_in_folder(path)\n",
    "    if folder in ['bending1', 'bending2']:\n",
    "        test_datasets = datasets[0:2]\n",
    "    else:\n",
    "        test_datasets = datasets[0:3]\n",
    "\n",
    "    for dataset in datasets:\n",
    "        dataset_path = os.path.join(path, dataset)\n",
    "        data = read_dataset_file(dataset_path)\n",
    "        \n",
    "        if data is not None:\n",
    "            dataset_name = os.path.join(folder, dataset)\n",
    "            if dataset in test_datasets:\n",
    "#                 print(f\"Test data for {dataset_name}:\",\"\\n\", data)\n",
    "                test_data.append(data)\n",
    "                test_data_paths.append(dataset_name)  \n",
    "            else:\n",
    "#                 print(f\"Train data for {dataset_name}:\",\"\\n\", data)\n",
    "                train_data.append(data)\n",
    "                train_data_paths.append(dataset_name)\n",
    "\n",
    "print(\"Datasets in train set:\",train_data_paths,\"\\n\")\n",
    "print(\"Datasets in test set:\",test_data_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### i. Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean: The average value of the time series data.   \n",
    "Standard Deviation: The variability of the data.   \n",
    "Minimum: The smallest value in the data.   \n",
    "Maximum: The largest value in the data.   \n",
    "Median: The middle value of the data.   \n",
    "Quartiles: Represent the 25th, 50th, and 75th percentiles.     \n",
    "Zero Crossing Rate: The number of times the time series goes from positive to negative or vice versa.   \n",
    "Peak Values: The local maximum values in the time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ii. Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraced features are:\n",
      "      min1   max1      mean1  median1      std1  1st_quartile1  3rd_quartile1  \\\n",
      "0   35.00  47.40  43.954500   44.330  1.558835          43.00        45.0000   \n",
      "1   33.00  47.75  42.179812   43.500  3.670666          39.15        45.0000   \n",
      "2   33.00  45.75  41.678063   41.750  2.243490          41.33        42.7500   \n",
      "3   37.00  48.00  43.454958   43.250  1.386098          42.50        45.0000   \n",
      "4   36.25  48.00  43.969125   44.500  1.618364          43.31        44.6700   \n",
      "..    ...    ...        ...      ...       ...            ...            ...   \n",
      "83  35.50  46.25  43.174938   43.670  1.989052          42.50        44.5000   \n",
      "84  32.75  47.00  42.760562   44.500  3.398919          41.33        45.3725   \n",
      "85  19.33  43.50  34.227771   35.500  4.889576          30.50        37.7500   \n",
      "86  12.50  45.00  33.509729   34.125  4.850923          30.50        36.7500   \n",
      "87  15.00  46.75  34.660583   35.000  5.315110          31.00        38.2500   \n",
      "\n",
      "    min2   max2     mean2  ...      std5  1st_quartile5  3rd_quartile5  min6  \\\n",
      "0    0.0   1.70  0.426250  ...  1.999604        35.3625         36.500   0.0   \n",
      "1    0.0   3.00  0.696042  ...  3.849448        30.4575         36.330   0.0   \n",
      "2    0.0   2.83  0.535979  ...  2.411026        28.4575         31.250   0.0   \n",
      "3    0.0   1.58  0.378083  ...  2.488862        22.2500         24.000   0.0   \n",
      "4    0.0   1.50  0.413125  ...  3.318301        20.5000         23.750   0.0   \n",
      "..   ...    ...       ...  ...       ...            ...            ...   ...   \n",
      "83   0.0   2.12  0.506583  ...  2.983976        12.7500         16.500   0.0   \n",
      "84   0.0   3.34  0.486167  ...  4.296574        13.0000         18.565   0.0   \n",
      "85   0.0  14.50  3.995729  ...  3.092094        14.7500         18.670   0.0   \n",
      "86   0.0  13.05  4.450771  ...  3.133564        14.6275         18.750   0.0   \n",
      "87   0.0  13.44  4.200896  ...  3.155015        14.2500         18.500   0.0   \n",
      "\n",
      "    max6     mean6  median6      std6  1st_quartile6  3rd_quartile6  \n",
      "0   1.79  0.493292    0.430  0.513506          0.000          0.940  \n",
      "1   2.18  0.613521    0.500  0.524317          0.000          1.000  \n",
      "2   1.79  0.383292    0.430  0.389164          0.000          0.500  \n",
      "3   5.26  0.679646    0.500  0.622534          0.430          0.870  \n",
      "4   2.96  0.555312    0.490  0.487826          0.000          0.830  \n",
      "..   ...       ...      ...       ...            ...            ...  \n",
      "83  5.72  0.911979    0.830  0.666161          0.470          1.220  \n",
      "84  5.73  0.842271    0.710  0.722165          0.430          1.090  \n",
      "85  9.74  3.394125    3.100  1.792090          2.105          4.425  \n",
      "86  8.96  3.378479    3.085  1.787360          2.060          4.440  \n",
      "87  8.99  3.244396    3.000  1.630983          2.120          4.240  \n",
      "\n",
      "[88 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "def extract_features(data):\n",
    "    extracted_features = []\n",
    "    \n",
    "    for instance in data:\n",
    "        extracted_feature_values = []\n",
    "        for column in columns[1:]:\n",
    "            extracted_feature_values.extend([\n",
    "                instance[column].min(),\n",
    "                instance[column].max(),\n",
    "                instance[column].mean(),\n",
    "                instance[column].median(),\n",
    "                instance[column].std(),\n",
    "                instance[column].quantile(0.25),\n",
    "                instance[column].quantile(0.75)\n",
    "            ])\n",
    "        extracted_features.append(extracted_feature_values)\n",
    "        \n",
    "    return extracted_features\n",
    "\n",
    "stats_features = ['min', 'max', 'mean', 'median', 'std', '1st_quartile', '3rd_quartile']\n",
    "new_train_data = extract_features(train_data)\n",
    "new_test_data = extract_features(test_data)\n",
    "new_columns = [f\"{stats_feature}{i}\" for i in range(1, 7) for stats_feature in stats_features]\n",
    "train_set_df = pd.DataFrame(new_train_data, columns=new_columns)\n",
    "test_set_df = pd.DataFrame(new_test_data, columns=new_columns)\n",
    "combined_df = pd.concat([train_set_df, test_set_df], axis=0).reset_index(drop=True)\n",
    "print(\"Extraced features are:\\n\", combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### iii. Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min1             10.320357\n",
       "max1              4.394362\n",
       "mean1             5.336381\n",
       "median1           5.440269\n",
       "std1              1.775828\n",
       "1st_quartile1     6.154694\n",
       "3rd_quartile1     5.138925\n",
       "min2              0.000000\n",
       "max2              5.062729\n",
       "mean2             1.574198\n",
       "median2           1.412293\n",
       "std2              0.884137\n",
       "1st_quartile2     0.946386\n",
       "3rd_quartile2     2.125399\n",
       "min3              2.956462\n",
       "max3              4.875137\n",
       "mean3             4.008228\n",
       "median3           4.036396\n",
       "std3              0.946670\n",
       "1st_quartile3     4.220658\n",
       "3rd_quartile3     4.171628\n",
       "min4              0.000000\n",
       "max4              2.183625\n",
       "mean4             1.166178\n",
       "median4           1.145985\n",
       "std4              0.458283\n",
       "1st_quartile4     0.843405\n",
       "3rd_quartile4     1.552504\n",
       "min5              6.124001\n",
       "max5              5.741238\n",
       "mean5             5.675543\n",
       "median5           5.813782\n",
       "std5              1.024918\n",
       "1st_quartile5     6.096465\n",
       "3rd_quartile5     5.531720\n",
       "min6              0.045838\n",
       "max6              2.518921\n",
       "mean6             1.154889\n",
       "median6           1.086474\n",
       "std6              0.517651\n",
       "1st_quartile6     0.758687\n",
       "3rd_quartile6     1.523739\n",
       "Name: std, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.describe().loc['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90% confidence interval for feature min1 is: [8.823797252056373, 11.520756131381692]\n",
      "90% confidence interval for feature max1 is: [3.3002839841770983, 5.26932389068262]\n",
      "90% confidence interval for feature mean1 is: [4.671840602658305, 5.857032794334005]\n",
      "90% confidence interval for feature median1 is: [4.768593482009319, 5.975715766704536]\n",
      "90% confidence interval for feature std1 is: [1.5699375491348728, 1.9407313055372308]\n",
      "90% confidence interval for feature 1st_quartile1 is: [5.555809484125307, 6.604761771516757]\n",
      "90% confidence interval for feature 3rd_quartile1 is: [4.296459778173168, 5.802306233781353]\n",
      "90% confidence interval for feature min2 is: [0.0, 0.0]\n",
      "90% confidence interval for feature max2 is: [4.612111132811395, 5.370925664333078]\n",
      "90% confidence interval for feature mean2 is: [1.388558829214072, 1.6986485896888652]\n",
      "90% confidence interval for feature median2 is: [1.232180660197245, 1.5354676886501242]\n",
      "90% confidence interval for feature std2 is: [0.7980407626124899, 0.9367885758372494]\n",
      "90% confidence interval for feature 1st_quartile2 is: [0.8265569525143367, 1.0286263368513007]\n",
      "90% confidence interval for feature 3rd_quartile2 is: [1.8885947349480303, 2.279990543575387]\n",
      "90% confidence interval for feature min3 is: [2.7432963630312477, 3.0926297800317433]\n",
      "90% confidence interval for feature max3 is: [4.160615948310267, 5.4364877885926095]\n",
      "90% confidence interval for feature mean3 is: [3.4046973366895545, 4.46622117879591]\n",
      "90% confidence interval for feature median3 is: [3.407956213949038, 4.513568073143683]\n",
      "90% confidence interval for feature std3 is: [0.7554790508707071, 1.1128519760520201]\n",
      "90% confidence interval for feature 1st_quartile3 is: [3.6095232245562174, 4.675509362359299]\n",
      "90% confidence interval for feature 3rd_quartile3 is: [3.524474936022616, 4.662759824419111]\n",
      "90% confidence interval for feature min4 is: [0.0, 0.0]\n",
      "90% confidence interval for feature max4 is: [1.9635824005079483, 2.3497485286601116]\n",
      "90% confidence interval for feature mean4 is: [1.0685048369578767, 1.2159849833977217]\n",
      "90% confidence interval for feature median4 is: [1.050009331061019, 1.1952217980623587]\n",
      "90% confidence interval for feature std4 is: [0.4185204444369179, 0.4833704963027594]\n",
      "90% confidence interval for feature 1st_quartile4 is: [0.7706971024506161, 0.8856233151011437]\n",
      "90% confidence interval for feature 3rd_quartile4 is: [1.4272640699738235, 1.6176633244052068]\n",
      "90% confidence interval for feature min5 is: [4.417029121260915, 7.464750446627433]\n",
      "90% confidence interval for feature max5 is: [4.725723579227221, 6.535832718885329]\n",
      "90% confidence interval for feature mean5 is: [4.410480536128961, 6.703597942856486]\n",
      "90% confidence interval for feature median5 is: [4.504884726763196, 6.877795687385822]\n",
      "90% confidence interval for feature std5 is: [0.8078414650631904, 1.213278557870614]\n",
      "90% confidence interval for feature 1st_quartile5 is: [4.779964522842419, 7.148420307062877]\n",
      "90% confidence interval for feature 3rd_quartile5 is: [4.351860161089617, 6.496684050330012]\n",
      "90% confidence interval for feature min6 is: [0.0, 0.07802896990623481]\n",
      "90% confidence interval for feature max6 is: [2.242108467388935, 2.7524329501936973]\n",
      "90% confidence interval for feature mean6 is: [1.0562266105942169, 1.2072695954485175]\n",
      "90% confidence interval for feature median6 is: [0.9885929909984715, 1.1417900492477402]\n",
      "90% confidence interval for feature std6 is: [0.4755891357152832, 0.542591709833492]\n",
      "90% confidence interval for feature 1st_quartile6 is: [0.6853712575473472, 0.80219635777425]\n",
      "90% confidence interval for feature 3rd_quartile6 is: [1.3960374380589273, 1.591789179670253]\n"
     ]
    }
   ],
   "source": [
    "def bootstrap_confidence_interval(original_data):\n",
    "\n",
    "    sample_std_list = []\n",
    "\n",
    "    #number of samples is 10000\n",
    "    for _ in range(10000):\n",
    "        sample = np.random.choice(original_data, size=len(original_data), replace=True)\n",
    "        sample_std = sample.std()\n",
    "        sample_std_list.append(sample_std)\n",
    "\n",
    "    np_std_samples = np.array(sample_std_list)\n",
    "    lower_bound = np.percentile(np_std_samples, 5)\n",
    "    upper_bound = np.percentile(np_std_samples, 95)\n",
    "    \n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "confidence_intervals = {} #(feature, CI)\n",
    "for col in combined_df.columns:\n",
    "    lower_bound, upper_bound = bootstrap_confidence_interval(combined_df[col])\n",
    "    confidence_intervals[col] = [lower_bound, upper_bound]\n",
    "\n",
    "for feature, interval in confidence_intervals.items():\n",
    "    print(f\"90% confidence interval for feature {feature} is: {interval}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### iv. Select Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my opinion, the 3 most important time-domain features are **mean**, **min** and **max**. Choosing mean is because we often use mean to represent the typical case of a feature, and the mean is often used to represent the regular case of a random variable. Choosing min and max is because it can give us the range of the data, showing the lower and upper bound of the data(and sometimes, it can represent the extreme case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ISLR 3.7.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Linear Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for traning RSS, the cubic regression model will be lower, since it contains non-linear variables and has more predictors, so that is more flexible than a linear regression and can fit more complex shapes in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Linear Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear model will perform better than cubic regression model. For training data, the cubic regression will have less bias. However, if the true model is actually linear, which means the cubic regression which have more non-linear predictors will potentially overfit the model from training data, and result in higher test error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Not Linear Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cubic regression model will preform better than linear model in the aspect of training RSS, since it has non-linear variables and more predictors, which leads to less bias in training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Not Linear Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evidence is not sufficient to judge which model will perform better in the aspect of test RSS. If the non-linearity is mild and quite different from cubic regression, the linear model will perform better; however if the unknown pattern of data is quite similar to cubic relation, then cubic regression will perform better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ISLR 3.7.3 - Extra Practice "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ISLR 3.7.5 - Extra Practice "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "294.435px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "3c20c2d94d2527936fe0f3a300eb11db30fed84423423838e2f93b74eb7aaebc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
