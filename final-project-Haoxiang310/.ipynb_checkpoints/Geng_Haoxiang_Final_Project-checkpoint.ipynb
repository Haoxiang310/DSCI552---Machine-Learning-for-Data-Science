{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Geng_Haoxiang_Final_Project</h1></center>\n",
    "<br>\n",
    "\n",
    "|      Name      |  GitHub Username  |   USC ID   |\n",
    "|----------------|-------------------|------------|\n",
    "|  Haoxiang Geng |    Haoxiang310    | 8045015278 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/akina/anaconda3/lib/python3.11/site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-macos==2.15.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.59.3)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.15.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/akina/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/akina/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/akina/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/akina/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/akina/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/akina/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/akina/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/akina/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: opencv-python in /Users/akina/anaconda3/lib/python3.11/site-packages (4.8.1.78)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Users/akina/anaconda3/lib/python3.11/site-packages (from opencv-python) (1.24.3)\n",
      "Requirement already satisfied: tensorflow-metal in /Users/akina/anaconda3/lib/python3.11/site-packages (1.1.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-metal) (0.38.4)\n",
      "Requirement already satisfied: six>=1.15.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-metal) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install opencv-python\n",
    "!pip install tensorflow-metal\n",
    "import os\n",
    "import os.path as op\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import logging\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from skimage import io\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from PIL import Image \n",
    "import cv2\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.applications import EfficientNetB0, ResNet50, VGG16\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Data Exploration and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m data_head_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Find all subframe directories\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m subdirs \u001b[38;5;241m=\u001b[39m [Path(subdir\u001b[38;5;241m.\u001b[39mstem) \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m data_head_dir\u001b[38;5;241m.\u001b[39miterdir() \u001b[38;5;28;01mif\u001b[39;00m subdir\u001b[38;5;241m.\u001b[39mis_dir()]\n\u001b[1;32m     14\u001b[0m src_image_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(a_path\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[:\u001b[38;5;241m3\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m a_path \u001b[38;5;129;01min\u001b[39;00m subdirs]\n",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m data_head_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Find all subframe directories\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m subdirs \u001b[38;5;241m=\u001b[39m [Path(subdir\u001b[38;5;241m.\u001b[39mstem) \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m data_head_dir\u001b[38;5;241m.\u001b[39miterdir() \u001b[38;5;28;01mif\u001b[39;00m subdir\u001b[38;5;241m.\u001b[39mis_dir()]\n\u001b[1;32m     14\u001b[0m src_image_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(a_path\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[:\u001b[38;5;241m3\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m a_path \u001b[38;5;129;01min\u001b[39;00m subdirs]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/pathlib.py:931\u001b[0m, in \u001b[0;36mPath.iterdir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21miterdir\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    928\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Iterate over the files in this directory.  Does not yield any\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;124;03m    result for the special paths '.' and '..'.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 931\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    932\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_child_relpath(name)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/data'"
     ]
    }
   ],
   "source": [
    "# Logging configuration\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    datefmt='%H:%M:%S',\n",
    "                    format='%(asctime)s | %(levelname)-5s | %(module)-15s | %(message)s')\n",
    "\n",
    "IMAGE_SIZE = (299, 299)  # All images contained in this dataset are 299x299 (originally, to match Inception v3 input size)\n",
    "SEED = 17\n",
    "\n",
    "# Head directory containing all image subframes. Update with the relative path of your data directory\n",
    "data_head_dir = Path('../data/data')\n",
    "\n",
    "# Find all subframe directories\n",
    "subdirs = [Path(subdir.stem) for subdir in data_head_dir.iterdir() if subdir.is_dir()]\n",
    "src_image_ids = ['_'.join(a_path.name.split('_')[:3]) for a_path in subdirs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train/val/test subframe IDs\n",
    "def load_text_ids(file_path):\n",
    "    \"\"\"Simple helper to load all lines from a text file\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = [line.strip() for line in f.readlines()]\n",
    "    return lines\n",
    "\n",
    "# Load the subframe names for the three data subsets\n",
    "train_ids = load_text_ids('./train_source_images.txt')\n",
    "validate_ids = load_text_ids('./val_source_images.txt')\n",
    "test_ids = load_text_ids('./test_source_images.txt')\n",
    "\n",
    "# Generate a list containing the dataset split for the matching subdirectory names\n",
    "subdir_splits = []\n",
    "for src_id in src_image_ids:\n",
    "    if src_id in train_ids:\n",
    "        subdir_splits.append('train')\n",
    "    elif src_id in validate_ids:\n",
    "        subdir_splits.append('validate')\n",
    "    elif(src_id in test_ids):\n",
    "        subdir_splits.append('test')\n",
    "    else:\n",
    "        logging.warning(f'{src_id}: Did not find designated split in train/validate/test list.')\n",
    "        subdir_splits.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess(img_loc, label, is_training=True):\n",
    "    #only perform data augmentation to training set\n",
    "    def _inner_function(img_loc, label):\n",
    "        img_loc_str = img_loc.numpy().decode('utf-8')\n",
    "        label_str = label.numpy().decode('utf-8')\n",
    "        img = cv2.imread(img_loc_str)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (299, 299))\n",
    "        if is_training:\n",
    "            if random.random() > 0.5:\n",
    "                img = cv2.flip(img, 1) \n",
    "            if random.random() > 0.5:\n",
    "                img = cv2.flip(img, 0)\n",
    "            angle = random.randint(-15, 15)\n",
    "            M = cv2.getRotationMatrix2D((img.shape[1]//2, img.shape[0]//2), angle, 1)\n",
    "            img = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))\n",
    "            alpha = 1.0 + random.uniform(-0.3, 0.3) \n",
    "            beta = random.uniform(-30, 30)\n",
    "            img = cv2.convertScaleAbs(img, alpha=alpha, beta=beta)\n",
    "\n",
    "        img = tf.convert_to_tensor(img, dtype=tf.float32)\n",
    "        img = img / 255.0\n",
    "        img.set_shape([299, 299, 3])\n",
    "        label = tf.cast(1 if label_str == 'frost' else 0, tf.int32)\n",
    "        label.set_shape([])\n",
    "        return img, label\n",
    "\n",
    "    X, y = tf.py_function(_inner_function, [img_loc, label], [tf.float32, tf.int32])\n",
    "    X.set_shape([299, 299, 3])\n",
    "    y.set_shape([])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_subdir_data(dir_path, image_size, seed=None):\n",
    "    \n",
    "    \"\"\"Helper to create a TF dataset from each image subdirectory\"\"\"\n",
    "    \n",
    "    # Grab only the classes that (1) we want to keep and (2) exist in this directory\n",
    "    tile_dir = dir_path / Path('tiles')\n",
    "    label_dir = dir_path /Path('labels')\n",
    "    \n",
    "    loc_list = []\n",
    "    \n",
    "    for folder in os.listdir(tile_dir):\n",
    "        if os.path.isdir(os.path.join(tile_dir, folder)):\n",
    "            for file in os.listdir(os.path.join(tile_dir, folder)):\n",
    "                if file.endswith(\".png\"):\n",
    "                    loc_list.append((os.path.join(os.path.join(tile_dir, folder), file), folder))\n",
    "\n",
    "    return loc_list\n",
    "\n",
    "# Loop over all subframes, loading each into a list\n",
    "tf_data_train, tf_data_test, tf_data_val = [], [], []\n",
    "tf_dataset_train, tf_dataset_test, tf_dataset_val = [], [], []\n",
    "buffer_size = 64\n",
    "batch_size = 32\n",
    "\n",
    "for subdir, split in zip(subdirs, subdir_splits):\n",
    "    full_path = data_head_dir / subdir\n",
    "    if split == 'validate':\n",
    "        tf_data_val.extend(load_subdir_data(full_path, IMAGE_SIZE, SEED))\n",
    "    elif split == 'train':\n",
    "        tf_data_train.extend(load_subdir_data(full_path, IMAGE_SIZE, SEED))\n",
    "    elif split == 'test':\n",
    "        tf_data_test.extend(load_subdir_data(full_path, IMAGE_SIZE, SEED))\n",
    "\n",
    "random.shuffle(tf_data_train)\n",
    "img_list, label_list = zip(*tf_data_train)\n",
    "img_list_t = tf.convert_to_tensor(img_list)\n",
    "lb_list_t = tf.convert_to_tensor(label_list)\n",
    "tf_dataset_train = tf.data.Dataset.from_tensor_slices((img_list_t, lb_list_t))\n",
    "tf_dataset_train = tf_dataset_train.map(lambda img, lbl: load_and_preprocess(img, lbl, True), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "tf_dataset_train = tf_dataset_train.shuffle(buffer_size=buffer_size).batch(batch_size)\n",
    "random.shuffle(tf_data_val)\n",
    "img_list, label_list = zip(*tf_data_val)\n",
    "img_list_t = tf.convert_to_tensor(img_list)\n",
    "lb_list_t = tf.convert_to_tensor(label_list)\n",
    "tf_dataset_val = tf.data.Dataset.from_tensor_slices((img_list_t, lb_list_t))\n",
    "tf_dataset_val = tf_dataset_val.map(lambda img, lbl: load_and_preprocess(img, lbl, False), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "tf_dataset_val = tf_dataset_val.shuffle(buffer_size=buffer_size).batch(batch_size)\n",
    "random.shuffle(tf_data_test)\n",
    "img_list, label_list = zip(*tf_data_test)\n",
    "img_list_t = tf.convert_to_tensor(img_list)\n",
    "lb_list_t = tf.convert_to_tensor(label_list)\n",
    "tf_dataset_test = tf.data.Dataset.from_tensor_slices((img_list_t, lb_list_t))\n",
    "tf_dataset_test = tf_dataset_test.map(lambda img, lbl: load_and_preprocess(img, lbl, False), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "tf_dataset_test = tf_dataset_test.shuffle(buffer_size=buffer_size).batch(batch_size)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches_train = tf.data.experimental.cardinality(tf_dataset_train).numpy()\n",
    "num_batches_val = tf.data.experimental.cardinality(tf_dataset_val).numpy()\n",
    "num_batches_test = tf.data.experimental.cardinality(tf_dataset_test).numpy()\n",
    "\n",
    "total_tiles_train = num_batches_train * batch_size\n",
    "total_tiles_val = num_batches_val * batch_size\n",
    "total_tiles_test = num_batches_test * batch_size\n",
    "\n",
    "print(f\"Total number of tiles in the training dataset: {total_tiles_train}\")\n",
    "print(f\"Total number of tiles in the validation dataset: {total_tiles_val}\")\n",
    "print(f\"Total number of tiles in the test dataset: {total_tiles_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (C) Training CNN + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=(299, 299, 3)))\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_plot(model, history, test_dataset, title, class_names=['Background', 'Frost']):\n",
    "    y_pred = model.predict(test_dataset)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_true = []\n",
    "    for _, label in test_dataset.unbatch():\n",
    "        y_true.append(label.numpy())\n",
    "    y_true = np.array(y_true)\n",
    "    report = classification_report(y_true, y_pred, target_names=class_names)\n",
    "    print(report)\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title(f'{title} - Accuracy over epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.title(f'{title} - Loss over epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(tf_dataset_train, \n",
    "                    epochs=20, \n",
    "                    validation_data=tf_dataset_val,\n",
    "                    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)])\n",
    "evaluate_and_plot(model, history, tf_dataset_test, 'CNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transfer_model(model_name):\n",
    "    if model_name == 'EfficientNetB0':\n",
    "        base_model = EfficientNetB0(include_top=False, input_shape=(299, 299, 3), weights='imagenet')\n",
    "    elif model_name == 'ResNet50':\n",
    "        base_model = ResNet50(include_top=False, input_shape=(299, 299, 3), weights='imagenet')\n",
    "    elif model_name == 'VGG16':\n",
    "        base_model = VGG16(include_top=False, input_shape=(299, 299, 3), weights='imagenet')\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model name\")\n",
    "\n",
    "    # freeze\n",
    "    base_model.trainable = False \n",
    "\n",
    "    inputs = tf.keras.Input(shape=(299, 299, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "model_efficientnet = create_transfer_model('EfficientNetB0')\n",
    "\n",
    "history_efficientnet = model_efficientnet.fit(tf_dataset_train, epochs=20, validation_data=tf_dataset_val, callbacks=[early_stopping_cb])\n",
    "\n",
    "evaluate_and_plot(model_efficientnet, history_efficientnet, tf_dataset_test, 'EfficientNetB0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet = create_transfer_model('ResNet50')\n",
    "\n",
    "history_resnet = model_resnet.fit(tf_dataset_train, epochs=20, validation_data=tf_dataset_val, callbacks=[early_stopping_cb])\n",
    "\n",
    "evaluate_and_plot(model_resnet, history_resnet, tf_dataset_test, 'ResNet50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg16 = create_transfer_model('VGG16')\n",
    "\n",
    "history_vgg16 = model_vgg16.fit(tf_dataset_train, epochs=20, validation_data=tf_dataset_val, callbacks=[early_stopping_cb])\n",
    "\n",
    "evaluate_and_plot(model_vgg16, history_vgg16, tf_dataset_test, 'VGG16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (vi).Compare results: \n",
    "From the perspective of training accuracy and test accuracy, transfer learning should outperform CNN + MLP in general. This is because these networks have already learned robust and complex feature representations from large and diverse datasets, typically on tasks like ImageNet classification. When applying transfer learning, this rich feature understanding is adapted to new, possibly smaller, datasets, providing a head start in learning. However, in practice, within transfer learning approaches, VGG16 and ResNet50 outperform CNN (VGG16 is the best, followed by ResNet50), but efficientnet performs slightly worse than CNN. This might be due to the nature of dataset, and efficientnet is not quite suitable on this kind of dataset, which correspond to the no free lunch theorem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
