{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Geng_Haoxiang_Final_Project</h1></center>\n",
    "<br>\n",
    "\n",
    "|      Name      |  GitHub Username  |   USC ID   |\n",
    "|----------------|-------------------|------------|\n",
    "|  Haoxiang Geng |    Haoxiang310    | 8045015278 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/akina/anaconda3/lib/python3.11/site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-macos==2.15.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.59.3)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.15.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/akina/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/akina/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/akina/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/akina/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/akina/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/akina/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/akina/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/akina/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: opencv-python in /Users/akina/anaconda3/lib/python3.11/site-packages (4.8.1.78)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Users/akina/anaconda3/lib/python3.11/site-packages (from opencv-python) (1.24.3)\n",
      "Requirement already satisfied: tensorflow-metal in /Users/akina/anaconda3/lib/python3.11/site-packages (1.1.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-metal) (0.38.4)\n",
      "Requirement already satisfied: six>=1.15.0 in /Users/akina/anaconda3/lib/python3.11/site-packages (from tensorflow-metal) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install opencv-python\n",
    "!pip install tensorflow-metal\n",
    "import os\n",
    "import os.path as op\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import logging\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from skimage import io\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from PIL import Image \n",
    "import cv2\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.applications import EfficientNetB0, ResNet50, VGG16\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Data Exploration and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging configuration\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    datefmt='%H:%M:%S',\n",
    "                    format='%(asctime)s | %(levelname)-5s | %(module)-15s | %(message)s')\n",
    "\n",
    "IMAGE_SIZE = (299, 299)  # All images contained in this dataset are 299x299 (originally, to match Inception v3 input size)\n",
    "SEED = 17\n",
    "\n",
    "# Head directory containing all image subframes. Update with the relative path of your data directory\n",
    "data_head_dir = Path('./data')\n",
    "\n",
    "# Find all subframe directories\n",
    "subdirs = [Path(subdir.stem) for subdir in data_head_dir.iterdir() if subdir.is_dir()]\n",
    "src_image_ids = ['_'.join(a_path.name.split('_')[:3]) for a_path in subdirs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train/val/test subframe IDs\n",
    "def load_text_ids(file_path):\n",
    "    \"\"\"Simple helper to load all lines from a text file\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = [line.strip() for line in f.readlines()]\n",
    "    return lines\n",
    "\n",
    "# Load the subframe names for the three data subsets\n",
    "train_ids = load_text_ids('./train_source_images.txt')\n",
    "validate_ids = load_text_ids('./val_source_images.txt')\n",
    "test_ids = load_text_ids('./test_source_images.txt')\n",
    "\n",
    "# Generate a list containing the dataset split for the matching subdirectory names\n",
    "subdir_splits = []\n",
    "for src_id in src_image_ids:\n",
    "    if src_id in train_ids:\n",
    "        subdir_splits.append('train')\n",
    "    elif src_id in validate_ids:\n",
    "        subdir_splits.append('validate')\n",
    "    elif(src_id in test_ids):\n",
    "        subdir_splits.append('test')\n",
    "    else:\n",
    "        logging.warning(f'{src_id}: Did not find designated split in train/validate/test list.')\n",
    "        subdir_splits.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-08 18:20:16.503151: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2023-12-08 18:20:16.503210: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2023-12-08 18:20:16.503250: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2023-12-08 18:20:16.503332: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-12-08 18:20:16.503362: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "def load_and_preprocess(img_loc, label):\n",
    "    def _inner_function(img_loc, label):\n",
    "        \n",
    "        #data augmentation\n",
    "        img_loc_str = img_loc.numpy().decode('utf-8')\n",
    "        label_str = label.numpy().decode('utf-8')\n",
    "        img = cv2.imread(img_loc_str)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (299, 299))\n",
    "        if random.random() > 0.5:\n",
    "            img = cv2.flip(img, 1) \n",
    "        if random.random() > 0.5:\n",
    "            img = cv2.flip(img, 0)\n",
    "        angle = random.randint(-15, 15)\n",
    "        M = cv2.getRotationMatrix2D((img.shape[1]//2, img.shape[0]//2), angle, 1)\n",
    "        img = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))\n",
    "        alpha = 1.0 + random.uniform(-0.3, 0.3) \n",
    "        beta = random.uniform(-30, 30)\n",
    "        img = cv2.convertScaleAbs(img, alpha=alpha, beta=beta)\n",
    "        img = tf.convert_to_tensor(img, dtype=tf.float32)\n",
    "        img = img / 255.0\n",
    "        img.set_shape([299, 299, 3])\n",
    "        label = tf.cast(1 if label_str == 'frost' else 0, tf.int32)\n",
    "        label.set_shape([])\n",
    "        return img, label\n",
    "\n",
    "    X, y = tf.py_function(_inner_function, [img_loc, label], [tf.float32, tf.int32])\n",
    "    X.set_shape([299, 299, 3])\n",
    "    y.set_shape([])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "def load_subdir_data(dir_path, image_size, seed=None):\n",
    "    \n",
    "    \"\"\"Helper to create a TF dataset from each image subdirectory\"\"\"\n",
    "    \n",
    "    # Grab only the classes that (1) we want to keep and (2) exist in this directory\n",
    "    tile_dir = dir_path / Path('tiles')\n",
    "    label_dir = dir_path /Path('labels')\n",
    "    \n",
    "    loc_list = []\n",
    "    \n",
    "    for folder in os.listdir(tile_dir):\n",
    "        if os.path.isdir(os.path.join(tile_dir, folder)):\n",
    "            for file in os.listdir(os.path.join(tile_dir, folder)):\n",
    "                if file.endswith(\".png\"):\n",
    "                    loc_list.append((os.path.join(os.path.join(tile_dir, folder), file), folder))\n",
    "\n",
    "    return loc_list\n",
    "\n",
    "# Loop over all subframes, loading each into a list\n",
    "tf_data_train, tf_data_test, tf_data_val = [], [], []\n",
    "tf_dataset_train, tf_dataset_test, tf_dataset_val = [], [], []\n",
    "\n",
    "# Update the batch and buffer size as per your model requirements\n",
    "buffer_size = 64\n",
    "batch_size = 32\n",
    "\n",
    "for subdir, split in zip(subdirs, subdir_splits):\n",
    "    full_path = data_head_dir / subdir\n",
    "    if split=='validate':\n",
    "        tf_data_val.extend(load_subdir_data(full_path, IMAGE_SIZE, SEED))\n",
    "    elif split=='train':\n",
    "        tf_data_train.extend(load_subdir_data(full_path, IMAGE_SIZE, SEED))\n",
    "    elif split=='test':\n",
    "        tf_data_test.extend(load_subdir_data(full_path, IMAGE_SIZE, SEED))\n",
    "        \n",
    "random.shuffle(tf_data_train)\n",
    "img_list, label_list = zip(*tf_data_train)\n",
    "img_list_t = tf.convert_to_tensor(img_list)\n",
    "lb_list_t = tf.convert_to_tensor(label_list)\n",
    "\n",
    "tf_dataset_train = tf.data.Dataset.from_tensor_slices((img_list_t, lb_list_t))\n",
    "tf_dataset_train = tf_dataset_train.map(load_and_preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "tf_dataset_train = tf_dataset_train.shuffle(buffer_size=buffer_size).batch(batch_size) \n",
    "\n",
    "random.shuffle(tf_data_val)\n",
    "img_list, label_list = zip(*tf_data_val)\n",
    "img_list_t = tf.convert_to_tensor(img_list)\n",
    "lb_list_t = tf.convert_to_tensor(label_list)\n",
    "\n",
    "tf_dataset_val = tf.data.Dataset.from_tensor_slices((img_list_t, lb_list_t))\n",
    "tf_dataset_val = tf_dataset_val.map(load_and_preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "tf_dataset_val = tf_dataset_val.shuffle(buffer_size=buffer_size).batch(batch_size) \n",
    "\n",
    "random.shuffle(tf_data_test)\n",
    "img_list, label_list = zip(*tf_data_test)\n",
    "img_list_t = tf.convert_to_tensor(img_list)\n",
    "lb_list_t = tf.convert_to_tensor(label_list)\n",
    "\n",
    "tf_dataset_test = tf.data.Dataset.from_tensor_slices((img_list_t, lb_list_t))\n",
    "tf_dataset_test = tf_dataset_test.map(load_and_preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "tf_dataset_test = tf_dataset_test.shuffle(buffer_size=buffer_size).batch(batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tiles in the training dataset: 29696\n",
      "Total number of tiles in the validation dataset: 11296\n",
      "Total number of tiles in the test dataset: 12832\n"
     ]
    }
   ],
   "source": [
    "num_batches_train = tf.data.experimental.cardinality(tf_dataset_train).numpy()\n",
    "num_batches_val = tf.data.experimental.cardinality(tf_dataset_val).numpy()\n",
    "num_batches_test = tf.data.experimental.cardinality(tf_dataset_test).numpy()\n",
    "\n",
    "total_tiles_train = num_batches_train * batch_size\n",
    "total_tiles_val = num_batches_val * batch_size\n",
    "total_tiles_test = num_batches_test * batch_size\n",
    "\n",
    "print(f\"Total number of tiles in the training dataset: {total_tiles_train}\")\n",
    "print(f\"Total number of tiles in the validation dataset: {total_tiles_val}\")\n",
    "print(f\"Total number of tiles in the test dataset: {total_tiles_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (C) Training CNN + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=(299, 299, 3)))\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_plot(model, history, test_dataset, title, class_names=['Background', 'Frost']):\n",
    "    y_pred = model.predict(test_dataset)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_true = []\n",
    "    for _, label in test_dataset.unbatch():\n",
    "        y_true.append(label.numpy())\n",
    "    y_true = np.array(y_true)\n",
    "    report = classification_report(y_true, y_pred, target_names=class_names)\n",
    "    print(report)\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title(f'{title} - Accuracy over epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.title(f'{title} - Loss over epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-08 18:20:16.993591: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "928/928 [==============================] - 152s 164ms/step - loss: 0.7767 - accuracy: 0.5737 - val_loss: 0.7394 - val_accuracy: 0.3218\n",
      "Epoch 2/20\n",
      "928/928 [==============================] - 152s 164ms/step - loss: 0.6828 - accuracy: 0.5795 - val_loss: 0.6950 - val_accuracy: 0.3297\n",
      "Epoch 3/20\n",
      "928/928 [==============================] - 152s 164ms/step - loss: 0.6618 - accuracy: 0.5984 - val_loss: 0.6768 - val_accuracy: 0.5933\n",
      "Epoch 4/20\n",
      "928/928 [==============================] - 152s 163ms/step - loss: 0.6572 - accuracy: 0.6045 - val_loss: 0.6999 - val_accuracy: 0.3744\n",
      "Epoch 5/20\n",
      "928/928 [==============================] - 1085s 1s/step - loss: 0.6553 - accuracy: 0.6090 - val_loss: 0.6818 - val_accuracy: 0.5269\n",
      "Epoch 6/20\n",
      "642/928 [===================>..........] - ETA: 40s - loss: 0.6570 - accuracy: 0.6083"
     ]
    }
   ],
   "source": [
    "history = model.fit(tf_dataset_train, \n",
    "                    epochs=20, \n",
    "                    validation_data=tf_dataset_val,\n",
    "                    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)])\n",
    "evaluate_and_plot(model, history, tf_dataset_test, 'CNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transfer_model(model_name):\n",
    "    if model_name == 'EfficientNetB0':\n",
    "        base_model = EfficientNetB0(include_top=False, input_shape=(299, 299, 3), weights='imagenet')\n",
    "    elif model_name == 'ResNet50':\n",
    "        base_model = ResNet50(include_top=False, input_shape=(299, 299, 3), weights='imagenet')\n",
    "    elif model_name == 'VGG16':\n",
    "        base_model = VGG16(include_top=False, input_shape=(299, 299, 3), weights='imagenet')\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model name\")\n",
    "\n",
    "    # freeze\n",
    "    base_model.trainable = False \n",
    "\n",
    "    inputs = tf.keras.Input(shape=(299, 299, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "model_efficientnet = create_transfer_model('EfficientNetB0')\n",
    "\n",
    "history_efficientnet = model_efficientnet.fit(tf_dataset_train, epochs=20, validation_data=tf_dataset_val, callbacks=[early_stopping_cb])\n",
    "\n",
    "evaluate_and_plot(model_efficientnet, history_efficientnet, tf_dataset_test, 'EfficientNetB0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet = create_transfer_model('ResNet50')\n",
    "\n",
    "history_resnet = model_resnet.fit(tf_dataset_train, epochs=20, validation_data=tf_dataset_val, callbacks=[early_stopping_cb])\n",
    "\n",
    "evaluate_and_plot(model_resnet, history_resnet, tf_dataset_test, 'ResNet50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg16 = create_transfer_model('VGG16')\n",
    "\n",
    "history_vgg16 = model_vgg16.fit(tf_dataset_train, epochs=20, validation_data=tf_dataset_val, callbacks=[early_stopping_cb])\n",
    "\n",
    "evaluate_and_plot(model_vgg16, history_vgg16, tf_dataset_test, 'VGG16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (vi).Compare results: \n",
    "From the perspective of training accuracy and test accuracy, transfer learning outperforms CNN + MLP. Within transfer learning approaches, VGG16 is the best, followed by ResNet50, with EfficientNet being the least effective. This is because transfer learning is able to leverage pre-trained networks. These networks have already learned robust and complex feature representations from large and diverse datasets, typically on tasks like ImageNet classification. When applying transfer learning, this rich feature understanding is adapted to new, possibly smaller, datasets, providing a head start in learning. In contrast, a basic three-layer CNN+MLP model starts learning from scratch, which can be less efficient and effective, especially with limited data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
